{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cce7593-7da1-4515-ac4a-48b1dfb896ca",
   "metadata": {},
   "source": [
    "# Azure Machine Learning Pipeline with Databricks Step\n",
    "Sample notebook demonstrating creation of an AML pipeline with a `DatabricksStep` that accepts two file datasets as inputs (one dataset is read from, the other written to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf93aca-2d8a-46de-9a55-cfdd28676ae0",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98ee8e-fd44-4139-b1c8-95ea21d943d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DatabricksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.pipeline.core.pipeline_output_dataset import PipelineOutputTabularDataset\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269542b-b3f1-4525-b5a2-3f400398c418",
   "metadata": {},
   "source": [
    "### Connect to AML workspace, create compute cluster, and get reference to default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4c8ef-9700-4ef3-8433-da9787a09cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df25c18d-4cd4-4f75-a27e-587d1ecf0d14",
   "metadata": {},
   "source": [
    "### Select a curated AML environment for pipeline execution\n",
    "Retrieve an existing curated environment from AML and attach to a `RunConfiguration` to support pipeline execution. Alternatively, custom environments can be created and attached here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f6c00-aaba-4d37-b0f8-ed405d9c2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_env = Environment.get(ws, 'AzureML-sklearn-1.0-ubuntu20.04-py38-cpu')\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment  = run_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b6f9b-3cda-493e-92e8-def76a4dfcc2",
   "metadata": {},
   "source": [
    "### Define output datasets\n",
    "Here we define two datasets `input_data` and `output_data` and define where these data should reside within our default datastore. These essentially become locations where data can be staged throughtout pipeline execution, and the provided path ensures that outputs will be overwritten on each pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd13ed-b8e7-4df5-80a0-f342a071941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = OutputFileDatasetConfig(name='input', destination=(default_ds, 'sfct/input_data'))\n",
    "output_data = OutputFileDatasetConfig(name='output', destination=(default_ds, 'sfct/output_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76984664-a804-41d1-80e9-daba83a29637",
   "metadata": {},
   "source": [
    "### Define DatabricksCompute\n",
    "`DatabricksCompute` can be defined using the code below. See [this document](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.databrickscompute?view=azure-ml-py) for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd978d-8464-4d11-88be-9b7c71c65297",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_compute_name=os.getenv(\"DATABRICKS_COMPUTE_NAME\", \"<YOUR-DATABRICKS-COMPUTE-NAME>\") # Databricks compute name\n",
    "db_resource_group=os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"<YOUR-RESOURCE-GROUP>\") # Databricks resource group\n",
    "db_workspace_name=os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"<YOUR-DATABRICKS-WORKSPACE-NAME>\") # Databricks workspace name\n",
    "db_access_token=os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"<YOUR-DATABRICKS-ACCESS-TOKEN>\") # Databricks access token\n",
    " \n",
    "try:\n",
    "    databricks_compute = DatabricksCompute(workspace=ws, name=db_compute_name)\n",
    "    print('Compute target {} already exists'.format(db_compute_name))\n",
    "except ComputeTargetException:\n",
    "    print('Compute not found, will use below parameters to attach new one')\n",
    "    print('db_compute_name {}'.format(db_compute_name))\n",
    "    print('db_resource_group {}'.format(db_resource_group))\n",
    "    print('db_workspace_name {}'.format(db_workspace_name))\n",
    "    print('db_access_token {}'.format(db_access_token))\n",
    " \n",
    "    config = DatabricksCompute.attach_configuration(\n",
    "        resource_group = db_resource_group,\n",
    "        workspace_name = db_workspace_name,\n",
    "        access_token= db_access_token)\n",
    "    databricks_compute=ComputeTarget.attach(ws, db_compute_name, config)\n",
    "    databricks_compute.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4823f7c-1022-491e-9403-c5f642ce4ecb",
   "metadata": {},
   "source": [
    "### Define pipeline steps\n",
    "Below we define the steps which should be executed over the course of our pipeline run. Two `PythonScriptSteps` are run, (Step One and Step Three) and a Databricks notebook is executed in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac515a-bcb2-4fa2-b309-2ca5567fdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_one = PythonScriptStep(\n",
    "    name='Step One',\n",
    "    script_name='step_one.py',\n",
    "    arguments =['--input_data', input_data, '--output_data', output_data],\n",
    "    outputs=[input_data, output_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "step_two = DatabricksStep(\n",
    "    name=\"Step Two - Databricks\",\n",
    "    inputs=[input_data.as_input(name='input'), output_data.as_input(name='output')],\n",
    "    num_workers=1,\n",
    "    notebook_path='<PATH-TO-NOTEBOOK>',\n",
    "    run_name='DBX_Notebook',\n",
    "    compute_target=databricks_compute,\n",
    "    existing_cluster_id='<DATABRICKS_CLUSTER_ID>',\n",
    "    permit_cluster_restart=False,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "step_three = PythonScriptStep(\n",
    "    name='Step Three',\n",
    "    script_name='step_three.py',\n",
    "    inputs=[output_data.as_input(name='output_data').as_download('./output_data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=True,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "step_three.run_after(step_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf8ab3e-2646-4efb-a82c-50873d17dbd8",
   "metadata": {},
   "source": [
    "### Define pipeline\n",
    "Below is the syntax for creating and running an Azure ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ec313-e51e-414c-a9d4-d73ad4bffa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [step_one, step_two, step_three]\n",
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline_run = Experiment(ws, 'DBX_Test').submit(pipeline)\n",
    "\n",
    "pipeline_run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
